@Article{zaranis-etal-2025-movie-arxiv,
      title={Movie Facts and Fibs (MF$^2$): A Benchmark for Long Movie Understanding}, 
      author={Emmanouil Zaranis and Ant\'onio Farinhas and Saul Santos and Beatriz Canaverde and Miguel Moura Ramos and Aditya K Surikuchi and Andr\'e Viveiros and Baohao Liao and Elena Bueno-Benito and Nithin Sivakumaran and Pavlo Vasylenko and Shoubin Yu and Sonal Sannigrahi and Wafaa Mohammed and Ben Peters and Danae Sánchez Villegas and Elias Stengel-Eskin and Giuseppe Attanasio and Jaehong Yoon and Stella Frank and Alessandro Suglia and Chrysoula Zerva and Desmond Elliott and Mariella Dimiccoli and Mohit Bansal and Oswald Lanz and Raffaella Bernardi and Raquel Fern\'andez and Sandro Pezzelle and Vlad Niculae and André F. T. Martins},
      year={Preprints},
      journal = {ArXiv},
      note = {2025.},
      url={https://arxiv.org/abs/2506.06275}, 
      abstract = {Despite recent progress in vision-language models (VLMs), holistic understanding of long-form video content remains a significant challenge, partly due to limitations in current benchmarks. Many focus on peripheral, ``needle-in-a-haystack'' details, encouraging context-insensitive retrieval over deep comprehension. Others rely on large-scale, semi-automatically generated questions (often produced by language models themselves) that are easier for models to answer but fail to reflect genuine understanding. In this paper, we introduce MF^2, a new benchmark for evaluating whether models can comprehend, consolidate, and recall key narrative information from full-length movies (50-170 minutes long). MF2 includes over 50 full-length, open-licensed movies, each paired with manually constructed sets of claim pairs -- one true (fact) and one plausible but false (fib), totalling over 850 pairs. These claims target core narrative elements such as character motivations and emotions, causal chains, and event order, and refer to memorable moments that humans can recall without rewatching the movie. Instead of multiple-choice formats, we adopt a binary claim evaluation protocol: for each pair, models must correctly identify both the true and false claims. This reduces biases like answer ordering and enables a more precise assessment of reasoning. Our experiments demonstrate that both open-weight and closed state-of-the-art models fall well short of human performance, underscoring the relative ease of the task for humans and their superior ability to retain and reason over critical narrative information -- an ability current VLMs lack.}
}


@Article{qi-chen-etal-arxiv-2025,
  author = 	 {Jirui Qi and Shan Chen and Zidi Xiong and Raquel Fern\'{a}ndez and Danielle S. Bitterman and Arianna Bisazza},
  title = 	 {When Models Reason in Your Language: Controlling Thinking Trace Language Comes at the Cost of Accuracy},
  journal = 	 {ArXiv},
  year = 	 {Preprints},
  note = 	 {2025.},
  url = 	 {https://arxiv.org/abs/2505.22888},
  url-github = {https://github.com/Betswish/mCoT-XReasoning},
  abstract = 	 {Recent Large Reasoning Models (LRMs) with thinking traces have shown strong performance on English reasoning tasks. However, their ability to think in other languages is less studied. This capability is as important as answer accuracy for real world applications because users may find the reasoning trace useful for oversight only when it is expressed in their own language. We comprehensively evaluate two leading families of LRMs on our XReasoning benchmark and find that even the most advanced models often revert to English or produce fragmented reasoning in other languages, revealing a substantial gap in multilingual reasoning. Prompt based interventions that force models to reason in the users language improve readability and oversight but reduce answer accuracy, exposing an important trade off. We further show that targeted post training on just 100 examples mitigates this mismatch, though some accuracy loss remains. Our results highlight the limited multilingual reasoning capabilities of current LRMs and outline directions for future work.}
}

@Article{neplenbroek-etal-arxiv-2025,
  author = 	 {Vera Neplenbroek and Arianna Bisazza and Raquel Fern\'{a}ndez},
  title = 	 {Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization},
  journal = 	 {ArXiv},
  year = 	 {Preprints},
  note = 	 {2025.},
  url = 	 {https://arxiv.org/abs/2505.16467},
  url-github = {https://github.com/Veranep/implicit-personalization-stereotypes},
  abstract = 	 {Generative Large Language Models (LLMs) infer user's demographic information from subtle cues in the conversation -- a phenomenon called implicit personalization. Prior work has shown that such inferences can lead to lower quality responses for users assumed to be from minority groups, even when no demographic information is explicitly provided. In this work, we systematically explore how LLMs respond to stereotypical cues using controlled synthetic conversations, by analyzing the models' latent user representations through both model internals and generated answers to targeted user questions. Our findings reveal that LLMs do infer demographic attributes based on these stereotypical signals, which for a number of groups even persists when the user explicitly identifies with a different demographic group. Finally, we show that this form of stereotype-driven implicit personalization can be effectively mitigated by intervening on the model's internal representations using a trained linear probe to steer them toward the explicitly stated identity. Our results highlight the need for greater transparency and control in how LLMs represent user identity.}
}

@Article{Horst-etal-arxiv-2025,
  author = 	 {Nicola Horst and Davide Mazzaccara and Antonia Schmidt and Michael Sullivan and Filippo Momentè and Luca Franceschetti and Philipp Sadler and Sherzod Hakimov and Alberto Testoni and Raffaella Bernardi and Raquel Fern\'{a}ndez and Alexander Koller and Oliver Lemon and David Schlangen and Mario Giulianelli and Alessandro Suglia},
  title = 	 {Playpen: An Environment for Exploring Learning Through Conversational Interaction},
  journal = 	 {ArXiv},
  year = 	 {Preprints},
  note =         {2025.},
  url = 	 {http://arxiv.org/abs/2504.08590},
  url-github = {https://github.com/lm-playpen/playpen},
  abstract = 	 {Are we running out of learning signal? Predicting the next word in an existing text has turned out to be a powerful signal, at least at scale. But there are signs that we are running out of this resource. In recent months, interaction between learner and feedback-giver has come into focus, both for "alignment" (with a reward model judging the quality of instruction following attempts) and for improving "reasoning" (process- and outcome-based verifiers judging reasoning steps). In this paper, we explore to what extent synthetic interaction in what we call Dialogue Games -- goal-directed and rule-governed activities driven predominantly by verbal actions -- can provide a learning signal, and how this signal can be used. We introduce an environment for producing such interaction data (with the help of a Large Language Model as counterpart to the learner model), both offline and online. We investigate the effects of supervised fine-tuning on this data, as well as reinforcement learning setups such as DPO, and GRPO; showing that all of these approaches achieve some improvements in in-domain games, but only GRPO demonstrates the ability to generalise to out-of-domain games as well as retain competitive performance in reference-based tasks. We release the framework and the baseline training setups in the hope that this can foster research in this promising new direction.}
}

@article{Bavaresco-deHeerKloots-etal-arxiv-2025,
title={Modelling Multimodal Integration in Human Concept Processing with Vision-Language Models},
author={Anna Bavaresco and Marianne de Heer Kloots and Sandro Pezzelle and Raquel Fern\'{a}ndez},
journal = {ArXiv},
year={Preprints},
url={https://arxiv.org/abs/2407.17914},
note={2025.},
abstract={Text representations from language models have proven remarkably predictive of human neural activity involved in language processing, 
with the recent transformer-based models outperforming previous architectures in downstream tasks and prediction of brain responses. 
However, the word representations learnt by language-only models may be limited in that they lack sensory information from other modalities, 
which several cognitive and neuroscience studies showed to be reflected in human meaning representations. Here, we leverage current pre-trained 
vision-language models (VLMs) to investigate whether the integration of visuo-linguistic information they operate leads to representations 
that are more aligned with human brain activity than those obtained by models trained with language-only input. We focus on fMRI responses 
recorded while participants read concept words in the context of either a full sentence or a picture. Our results reveal that VLM representations 
correlate more strongly than those by language-only models with activations in brain areas functionally related to language processing. 
Additionally, we find that transformer-based vision-language encoders -- e.g., LXMERT and VisualBERT -- yield more brain-aligned representations 
than generative VLMs, whose autoregressive abilities do not seem to provide an advantage when modelling single words. Finally, our ablation 
analyses suggest that the high brain alignment achieved by some of the VLMs we evaluate results from semantic information acquired specifically 
during multimodal pretraining as opposed to being already encoded in their unimodal modules. Altogether, our findings indicate an advantage of 
multimodal models in predicting human brain activations, which reveals that modelling language and vision integration has the potential to 
capture the multimodal nature of human concept representations.}
}

@Article{Momente-etal-arxiv-2025,
  title={Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests}, 
  author={Filippo Moment\`{e} and Alessandro Suglia and Mario Giulianelli and Ambra Ferrari and Alexander Koller and Oliver Lemon and David Schlangen and Raquel Fern\'{a}ndez and Raffaella Bernardi},
  year={Preprints},
  journal = {ArXiv},
  note={2025.},
  url = {https://arxiv.org/abs/2502.14359},
  abstract = {We examine three evaluation paradigms: large question-answering benchmarks (e.g., MMLU and BBH), interactive games (e.g., Signalling Games or Taboo), and cognitive tests (e.g., for working memory or theory of mind). First, we investigate which of the former two-benchmarks or games-is most effective at discriminating LLMs of varying quality. Then, inspired by human cognitive assessments, we compile a suite of targeted tests that measure cognitive abilities deemed essential for effective language use, and we investigate their correlation with model performance in benchmarks and games. Our analyses reveal that interactive games are superior to standard benchmarks in discriminating models. Causal and logical reasoning correlate with both static and interactive tests, while differences emerge regarding core executive functions and social/emotional skills, which correlate more with games. We advocate the development of new interactive benchmarks and targeted cognitive tasks inspired by assessing human abilities but designed specifically for LLMs.}
}

@article{Testoni-etal-arxiv-2024,
  author = 	 {Alberto Testoni and Barbara Plank and Raquel Fern\'{a}ndez},
  title = 	 {RACQUET: Unveiling the Dangers of Overlooked Referential Ambiguity in Visual LLMs},
  journal = {ArXiv},
  year = 	 {Preprints},
  note={2024.},
  url={https://arxiv.org/abs/2412.13835},
  abstract={Ambiguity resolution is key to effective communication. While humans effortlessly address ambiguity through conversational grounding strategies, the extent to which current language models can emulate these strategies remains unclear. In this work, we examine referential ambiguity in image-based question answering by introducing RACQUET, a carefully curated dataset targeting distinct aspects of ambiguity. Through a series of evaluations, we reveal significant limitations and problems of overconfidence of state-of-the-art large multimodal language models in addressing ambiguity in their responses. The overconfidence issue becomes particularly relevant for RACQUET-BIAS, a subset designed to analyze a critical yet underexplored problem: failing to address ambiguity leads to stereotypical, socially biased responses. Our results underscore the urgency of equipping models with robust strategies to deal with uncertainty without resorting to undesirable stereotypes.}
}

@article{IAS-psyarxiv-2024,
title={Incremental Alternative Sampling as a Lens into the Temporal and Representational Resolution of Linguistic Prediction},
author={Mario Giulianelli and Sarenne Wallbridge and Ryan Cotterell and Raquel Fern\'{a}ndez},
journal = {PsyArXiv},
year={Preprints},
url={https://osf.io/preprints/psyarxiv/fhp84},
note={2024.},
abstract={This study presents a new model of processing difficulty rooted in resource allocation theory, Incremental Alternative Sampling (IAS). 
Differential difficulty for a linguistic unit is estimated with respect to a set of plausible alter-natives.  
Compared to a surprisal-based model, it prescribes a more efficient use of a comprehender’s predicted continuations of partial linguistic stimuli 
thanks to (i) an expressive representation function that captures different lev-els of linguistic processing and (ii) the bootstrapping of 
long-horizon prediction error.  Our results show that IAS estimates of processing difficulty, computed with autoregressive language models 
via Monte Carlo estimation, have greater  predictive  power  than  surprisal  extracted  from  the  same  language models for most neural 
and behavioural responses under analysis---including reading times, event-related brain potentials, cloze and predictability judgements.   
Perhaps  more  importantly,  IAS  estimates  provide  insight  into  the nature  of  the  predictive  mechanisms  that  generate  those  
responses  during language comprehension.  Variability in neural and behavioural responses is well  explained  by  different  combinations  
of  the  representational  and  temporal  resolution  of  prediction.   Processing  difficulty  calculated  at  varying representational 
domains reflects known relations to lexical, constructional, and structural levels of linguistic processing, and forecast horizons are 
determined by a combination of experimental task setup and naturalness of the stimulus.   Beyond  enriching  psycholinguistic  models,  
IAS  can  also  provide insights into the information processing mechanisms of computational language models.  
Our analysis of next-word surprisal under the lenses of IAS reveals that,  despite the metric’s seemingly narrow focus on the upcoming word, 
language model surprisal implicitly captures anticipatory processing of multiple future lexical items.}
}

@article{ghaleb-etal-2024-arxiv,
title={Leveraging Speech for Gesture Detection in Multimodal Communication}, 
author={Esam Ghaleb and Ilya Burenko and Marlou Rasenberg and Wim Pouw and Ivan Toni and Peter Uhrig and Anna Wilson and Judith Holler and Asl\i \"Ozy\"urek and Raquel Fern\'andez},
journal = {ArXiv},
year={Preprints},
url={https://arxiv.org/pdf/2404.14952},
note={2024.},
abstract={Gestures are inherent to human interaction and often complement speech in face-to-face communication, forming a multimodal 
communication system. An important task in gesture analysis is detecting a gesture's beginning and end. Research on automatic gesture 
detection has primarily focused on visual and kinematic information to detect a limited set of isolated or silent gestures with low 
variability, neglecting the integration of speech and vision signals to detect gestures that co-occur with speech. 
This work addresses this gap by focusing on co-speech gesture detection, emphasising the synchrony between speech and co-speech hand gestures. 
We address three main challenges: the variability of gesture forms, the temporal misalignment between gesture and speech onsets, 
and differences in sampling rate between modalities. We investigate extended speech time windows and employ separate backbone models 
for each modality to address the temporal misalignment and sampling rate differences. We utilize Transformer encoders in cross-modal 
and early fusion techniques to effectively align and integrate speech and skeletal sequences. The study results show that combining 
visual and speech information significantly enhances gesture detection performance. Our findings indicate that expanding the speech 
buffer beyond visual time segments improves performance and that multimodal integration using cross-modal and early fusion techniques 
outperforms baseline methods using unimodal and late fusion methods. Additionally, we find a correlation between the models' gesture 
prediction confidence and low-level speech frequency features potentially associated with gestures. Overall, the study provides a better 
understanding and detection methods for co-speech gestures, facilitating the analysis of multimodal communication.}
}

@article{baan-etal-2023-uncertaintyNLG-arxiv,
      title={Uncertainty in Natural Language Generation: From Theory to Applications}, 
      author={Joris Baan and Nico Daheim and Evgenia Ilia and Dennis Ulmer and Haau-Sing Li and Raquel Fern\'andez and Barbara Plank and Rico Sennrich and Chrysoula Zerva and Wilker Aziz},
      year={Preprints},
      journal = {ArXiv},
      url={https://arxiv.org/abs/2307.15703}, 
      note = {2023.},
      abstract = {Recent advances of powerful Language Models have allowed Natural Language Generation (NLG) to emerge as an important technology 
that can not only perform traditional tasks like summarisation or translation, but also serve as a natural language interface to a variety of applications. 
As such, it is crucial that NLG systems are trustworthy and reliable, for example by indicating when they are likely to be wrong; and supporting multiple views, 
backgrounds and writing styles -- reflecting diverse human sub-populations. In this paper, we argue that a principled treatment of uncertainty can assist in 
creating systems and evaluation protocols better aligned with these goals. We first present the fundamental theory, frameworks and vocabulary required to 
represent uncertainty. We then characterise the main sources of uncertainty in NLG from a linguistic perspective, and propose a two-dimensional taxonomy that 
is more informative and faithful than the popular aleatoric/epistemic dichotomy. Finally, we move from theory to applications and highlight exciting research directions that 
exploit uncertainty to power decoding, controllable generation, self-assessment, selective answering, active learning and more.}
}
