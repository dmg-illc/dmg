@Article{zaranis-etal-2025-movie-arxiv,
      title={Movie Facts and Fibs (MF^2): A Benchmark for Long Movie Understanding}, 
      author={Emmanouil Zaranis and Ant\'onio Farinhas and Saul Santos and Beatriz Canaverde and Miguel Moura Ramos and Aditya K Surikuchi and Andr\'e Viveiros and Baohao Liao and Elena Bueno-Benito and Nithin Sivakumaran and Pavlo Vasylenko and Shoubin Yu and Sonal Sannigrahi and Wafaa Mohammed and Ben Peters and Danae Sánchez Villegas and Elias Stengel-Eskin and Giuseppe Attanasio and Jaehong Yoon and Stella Frank and Alessandro Suglia and Chrysoula Zerva and Desmond Elliott and Mariella Dimiccoli and Mohit Bansal and Oswald Lanz and Raffaella Bernardi and Raquel Fern\'andez and Sandro Pezzelle and Vlad Niculae and André F. T. Martins},
      year={Preprints},
      journal = {ArXiv},
      note = {2025.},
      url={https://arxiv.org/abs/2506.06275}, 
      abstract = {Despite recent progress in vision-language models (VLMs), holistic understanding of long-form video content remains a significant challenge, partly due to limitations in current benchmarks. Many focus on peripheral, ``needle-in-a-haystack'' details, encouraging context-insensitive retrieval over deep comprehension. Others rely on large-scale, semi-automatically generated questions (often produced by language models themselves) that are easier for models to answer but fail to reflect genuine understanding. In this paper, we introduce MF^2, a new benchmark for evaluating whether models can comprehend, consolidate, and recall key narrative information from full-length movies (50-170 minutes long). MF2 includes over 50 full-length, open-licensed movies, each paired with manually constructed sets of claim pairs -- one true (fact) and one plausible but false (fib), totalling over 850 pairs. These claims target core narrative elements such as character motivations and emotions, causal chains, and event order, and refer to memorable moments that humans can recall without rewatching the movie. Instead of multiple-choice formats, we adopt a binary claim evaluation protocol: for each pair, models must correctly identify both the true and false claims. This reduces biases like answer ordering and enables a more precise assessment of reasoning. Our experiments demonstrate that both open-weight and closed state-of-the-art models fall well short of human performance, underscoring the relative ease of the task for humans and their superior ability to retain and reason over critical narrative information -- an ability current VLMs lack.}
}

@article{Bavaresco-deHeerKloots-etal-arxiv-2025,
title={Modelling Multimodal Integration in Human Concept Processing with Vision-Language Models},
author={Anna Bavaresco and Marianne de Heer Kloots and Sandro Pezzelle and Raquel Fern\'{a}ndez},
journal = {ArXiv},
year={Preprints},
url={https://arxiv.org/abs/2407.17914},
note={2025.},
abstract={Text representations from language models have proven remarkably predictive of human neural activity involved in language processing, 
with the recent transformer-based models outperforming previous architectures in downstream tasks and prediction of brain responses. 
However, the word representations learnt by language-only models may be limited in that they lack sensory information from other modalities, 
which several cognitive and neuroscience studies showed to be reflected in human meaning representations. Here, we leverage current pre-trained 
vision-language models (VLMs) to investigate whether the integration of visuo-linguistic information they operate leads to representations 
that are more aligned with human brain activity than those obtained by models trained with language-only input. We focus on fMRI responses 
recorded while participants read concept words in the context of either a full sentence or a picture. Our results reveal that VLM representations 
correlate more strongly than those by language-only models with activations in brain areas functionally related to language processing. 
Additionally, we find that transformer-based vision-language encoders -- e.g., LXMERT and VisualBERT -- yield more brain-aligned representations 
than generative VLMs, whose autoregressive abilities do not seem to provide an advantage when modelling single words. Finally, our ablation 
analyses suggest that the high brain alignment achieved by some of the VLMs we evaluate results from semantic information acquired specifically 
during multimodal pretraining as opposed to being already encoded in their unimodal modules. Altogether, our findings indicate an advantage of 
multimodal models in predicting human brain activations, which reveals that modelling language and vision integration has the potential to 
capture the multimodal nature of human concept representations.}
}

@article{IAS-psyarxiv-2024,
title={Incremental Alternative Sampling as a Lens into the Temporal and Representational Resolution of Linguistic Prediction},
author={Mario Giulianelli and Sarenne Wallbridge and Ryan Cotterell and Raquel Fern\'{a}ndez},
journal = {PsyArXiv},
year={Preprints},
url={https://osf.io/preprints/psyarxiv/fhp84},
note={2024.},
abstract={This study presents a new model of processing difficulty rooted in resource allocation theory, Incremental Alternative Sampling (IAS). 
Differential difficulty for a linguistic unit is estimated with respect to a set of plausible alter-natives.  
Compared to a surprisal-based model, it prescribes a more efficient use of a comprehender’s predicted continuations of partial linguistic stimuli 
thanks to (i) an expressive representation function that captures different lev-els of linguistic processing and (ii) the bootstrapping of 
long-horizon prediction error.  Our results show that IAS estimates of processing difficulty, computed with autoregressive language models 
via Monte Carlo estimation, have greater  predictive  power  than  surprisal  extracted  from  the  same  language models for most neural 
and behavioural responses under analysis---including reading times, event-related brain potentials, cloze and predictability judgements.   
Perhaps  more  importantly,  IAS  estimates  provide  insight  into  the nature  of  the  predictive  mechanisms  that  generate  those  
responses  during language comprehension.  Variability in neural and behavioural responses is well  explained  by  different  combinations  
of  the  representational  and  temporal  resolution  of  prediction.   Processing  difficulty  calculated  at  varying representational 
domains reflects known relations to lexical, constructional, and structural levels of linguistic processing, and forecast horizons are 
determined by a combination of experimental task setup and naturalness of the stimulus.   Beyond  enriching  psycholinguistic  models,  
IAS  can  also  provide insights into the information processing mechanisms of computational language models.  
Our analysis of next-word surprisal under the lenses of IAS reveals that,  despite the metric’s seemingly narrow focus on the upcoming word, 
language model surprisal implicitly captures anticipatory processing of multiple future lexical items.}
}

@article{ghaleb-etal-2024-arxiv,
title={Leveraging Speech for Gesture Detection in Multimodal Communication}, 
author={Esam Ghaleb and Ilya Burenko and Marlou Rasenberg and Wim Pouw and Ivan Toni and Peter Uhrig and Anna Wilson and Judith Holler and Asl\i \"Ozy\"urek and Raquel Fern\'andez},
journal = {ArXiv},
year={Preprints},
url={https://arxiv.org/pdf/2404.14952},
note={2024.},
abstract={Gestures are inherent to human interaction and often complement speech in face-to-face communication, forming a multimodal 
communication system. An important task in gesture analysis is detecting a gesture's beginning and end. Research on automatic gesture 
detection has primarily focused on visual and kinematic information to detect a limited set of isolated or silent gestures with low 
variability, neglecting the integration of speech and vision signals to detect gestures that co-occur with speech. 
This work addresses this gap by focusing on co-speech gesture detection, emphasising the synchrony between speech and co-speech hand gestures. 
We address three main challenges: the variability of gesture forms, the temporal misalignment between gesture and speech onsets, 
and differences in sampling rate between modalities. We investigate extended speech time windows and employ separate backbone models 
for each modality to address the temporal misalignment and sampling rate differences. We utilize Transformer encoders in cross-modal 
and early fusion techniques to effectively align and integrate speech and skeletal sequences. The study results show that combining 
visual and speech information significantly enhances gesture detection performance. Our findings indicate that expanding the speech 
buffer beyond visual time segments improves performance and that multimodal integration using cross-modal and early fusion techniques 
outperforms baseline methods using unimodal and late fusion methods. Additionally, we find a correlation between the models' gesture 
prediction confidence and low-level speech frequency features potentially associated with gestures. Overall, the study provides a better 
understanding and detection methods for co-speech gestures, facilitating the analysis of multimodal communication.}
}

@article{baan-etal-2023-uncertaintyNLG-arxiv,
      title={Uncertainty in Natural Language Generation: From Theory to Applications}, 
      author={Joris Baan and Nico Daheim and Evgenia Ilia and Dennis Ulmer and Haau-Sing Li and Raquel Fern\'andez and Barbara Plank and Rico Sennrich and Chrysoula Zerva and Wilker Aziz},
      year={Preprints},
      journal = {ArXiv},
      url={https://arxiv.org/abs/2307.15703}, 
      note = {2023.},
      abstract = {Recent advances of powerful Language Models have allowed Natural Language Generation (NLG) to emerge as an important technology 
that can not only perform traditional tasks like summarisation or translation, but also serve as a natural language interface to a variety of applications. 
As such, it is crucial that NLG systems are trustworthy and reliable, for example by indicating when they are likely to be wrong; and supporting multiple views, 
backgrounds and writing styles -- reflecting diverse human sub-populations. In this paper, we argue that a principled treatment of uncertainty can assist in 
creating systems and evaluation protocols better aligned with these goals. We first present the fundamental theory, frameworks and vocabulary required to 
represent uncertainty. We then characterise the main sources of uncertainty in NLG from a linguistic perspective, and propose a two-dimensional taxonomy that 
is more informative and faithful than the popular aleatoric/epistemic dichotomy. Finally, we move from theory to applications and highlight exciting research directions that 
exploit uncertainty to power decoding, controllable generation, self-assessment, selective answering, active learning and more.}
}
