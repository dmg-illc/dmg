

@Article{neplenbroek-etal-arxiv-2025,
  author = 	 {Vera Neplenbroek and Arianna Bisazza and Raquel Fern\'{a}ndez},
  title = 	 {Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization},
  journal = 	 {ArXiv},
  year = 	 {Preprints},
  note = 	 {2025},
  url = 	 {https://arxiv.org/abs/2505.16467},
  url-github = {https://github.com/Veranep/implicit-personalization-stereotypes},
  abstract = 	 {Generative Large Language Models (LLMs) infer user's demographic information from subtle cues in the conversation -- a phenomenon called implicit personalization. Prior work has shown that such inferences can lead to lower quality responses for users assumed to be from minority groups, even when no demographic information is explicitly provided. In this work, we systematically explore how LLMs respond to stereotypical cues using controlled synthetic conversations, by analyzing the models' latent user representations through both model internals and generated answers to targeted user questions. Our findings reveal that LLMs do infer demographic attributes based on these stereotypical signals, which for a number of groups even persists when the user explicitly identifies with a different demographic group. Finally, we show that this form of stereotype-driven implicit personalization can be effectively mitigated by intervening on the model's internal representations using a trained linear probe to steer them toward the explicitly stated identity. Our results highlight the need for greater transparency and control in how LLMs represent user identity.}
}

@Article{Horst-etal-arxiv-2025,
  author = 	 {Nicola Horst and Davide Mazzaccara and Antonia Schmidt and Michael Sullivan and Filippo Momentè and Luca Franceschetti and Philipp Sadler and Sherzod Hakimov and Alberto Testoni and Raffaella Bernardi and Raquel Fern\'{a}ndez and Alexander Koller and Oliver Lemon and David Schlangen and Mario Giulianelli and Alessandro Suglia},
  title = 	 {Playpen: An Environment for Exploring Learning Through Conversational Interaction},
  journal = 	 {ArXiv},
  year = 	 {Preprints},
  note =         {2025},
  url = 	 {http://arxiv.org/abs/2504.08590},
  url-github = {https://github.com/lm-playpen/playpen},
  abstract = 	 {Are we running out of learning signal? Predicting the next word in an existing text has turned out to be a powerful signal, at least at scale. But there are signs that we are running out of this resource. In recent months, interaction between learner and feedback-giver has come into focus, both for "alignment" (with a reward model judging the quality of instruction following attempts) and for improving "reasoning" (process- and outcome-based verifiers judging reasoning steps). In this paper, we explore to what extent synthetic interaction in what we call Dialogue Games -- goal-directed and rule-governed activities driven predominantly by verbal actions -- can provide a learning signal, and how this signal can be used. We introduce an environment for producing such interaction data (with the help of a Large Language Model as counterpart to the learner model), both offline and online. We investigate the effects of supervised fine-tuning on this data, as well as reinforcement learning setups such as DPO, and GRPO; showing that all of these approaches achieve some improvements in in-domain games, but only GRPO demonstrates the ability to generalise to out-of-domain games as well as retain competitive performance in reference-based tasks. We release the framework and the baseline training setups in the hope that this can foster research in this promising new direction.}
}

@article{Bavaresco-deHeerKloots-etal-arxiv-2025,
title={Modelling Multimodal Integration in Human Concept Processing with Vision-Language Models},
author={Anna Bavaresco and Marianne de Heer Kloots and Sandro Pezzelle and Raquel Fern\'{a}ndez},
journal = {ArXiv},
year={Preprints},
url={https://arxiv.org/abs/2407.17914},
note={2025.},
abstract={Text representations from language models have proven remarkably predictive of human neural activity involved in language processing, 
with the recent transformer-based models outperforming previous architectures in downstream tasks and prediction of brain responses. 
However, the word representations learnt by language-only models may be limited in that they lack sensory information from other modalities, 
which several cognitive and neuroscience studies showed to be reflected in human meaning representations. Here, we leverage current pre-trained 
vision-language models (VLMs) to investigate whether the integration of visuo-linguistic information they operate leads to representations 
that are more aligned with human brain activity than those obtained by models trained with language-only input. We focus on fMRI responses 
recorded while participants read concept words in the context of either a full sentence or a picture. Our results reveal that VLM representations 
correlate more strongly than those by language-only models with activations in brain areas functionally related to language processing. 
Additionally, we find that transformer-based vision-language encoders -- e.g., LXMERT and VisualBERT -- yield more brain-aligned representations 
than generative VLMs, whose autoregressive abilities do not seem to provide an advantage when modelling single words. Finally, our ablation 
analyses suggest that the high brain alignment achieved by some of the VLMs we evaluate results from semantic information acquired specifically 
during multimodal pretraining as opposed to being already encoded in their unimodal modules. Altogether, our findings indicate an advantage of 
multimodal models in predicting human brain activations, which reveals that modelling language and vision integration has the potential to 
capture the multimodal nature of human concept representations.}
}

@Article{Momente-etal-arxiv-2025,
  title={Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests}, 
  author={Filippo Moment\`{e} and Alessandro Suglia and Mario Giulianelli and Ambra Ferrari and Alexander Koller and Oliver Lemon and David Schlangen and Raquel Fern\'{a}ndez and Raffaella Bernardi},
  year={Preprints},
  journal = 	 {ArXiv},
  note={2025.},
  url = {https://arxiv.org/abs/2502.14359},
  abstract = {We examine three evaluation paradigms: large question-answering benchmarks (e.g., MMLU and BBH), interactive games (e.g., Signalling Games or Taboo), and cognitive tests (e.g., for working memory or theory of mind). First, we investigate which of the former two-benchmarks or games-is most effective at discriminating LLMs of varying quality. Then, inspired by human cognitive assessments, we compile a suite of targeted tests that measure cognitive abilities deemed essential for effective language use, and we investigate their correlation with model performance in benchmarks and games. Our analyses reveal that interactive games are superior to standard benchmarks in discriminating models. Causal and logical reasoning correlate with both static and interactive tests, while differences emerge regarding core executive functions and social/emotional skills, which correlate more with games. We advocate the development of new interactive benchmarks and targeted cognitive tasks inspired by assessing human abilities but designed specifically for LLMs.}
}

@article{Testoni-etal-arxiv-2024,
  author = 	 {Alberto Testoni and Barbara Plank and Raquel Fern\'{a}ndez},
  title = 	 {RACQUET: Unveiling the Dangers of Overlooked Referential Ambiguity in Visual LLMs},
  journal = 	 {ArXiv},
  year = 	 {Preprints},
  note={2024.},
  url={https://arxiv.org/abs/2412.13835},
  abstract={Ambiguity resolution is key to effective communication. While humans effortlessly address ambiguity through conversational grounding strategies, the extent to which current language models can emulate these strategies remains unclear. In this work, we examine referential ambiguity in image-based question answering by introducing RACQUET, a carefully curated dataset targeting distinct aspects of ambiguity. Through a series of evaluations, we reveal significant limitations and problems of overconfidence of state-of-the-art large multimodal language models in addressing ambiguity in their responses. The overconfidence issue becomes particularly relevant for RACQUET-BIAS, a subset designed to analyze a critical yet underexplored problem: failing to address ambiguity leads to stereotypical, socially biased responses. Our results underscore the urgency of equipping models with robust strategies to deal with uncertainty without resorting to undesirable stereotypes.}
}

@article{IAS-psyarxiv-2024,
title={Incremental Alternative Sampling as a Lens into the Temporal and Representational Resolution of Linguistic Prediction},
author={Mario Giulianelli and Sarenne Wallbridge and Ryan Cotterell and Raquel Fern\'{a}ndez},
journal = {PsyArXiv},
year={Preprints},
url={https://osf.io/preprints/psyarxiv/fhp84},
note={2024.},
abstract={This study presents a new model of processing difficulty rooted in resource allocation theory, Incremental Alternative Sampling (IAS). 
Differential difficulty for a linguistic unit is estimated with respect to a set of plausible alter-natives.  
Compared to a surprisal-based model, it prescribes a more efficient use of a comprehender’s predicted continuations of partial linguistic stimuli 
thanks to (i) an expressive representation function that captures different lev-els of linguistic processing and (ii) the bootstrapping of 
long-horizon prediction error.  Our results show that IAS estimates of processing difficulty, computed with autoregressive language models 
via Monte Carlo estimation, have greater  predictive  power  than  surprisal  extracted  from  the  same  language models for most neural 
and behavioural responses under analysis---including reading times, event-related brain potentials, cloze and predictability judgements.   
Perhaps  more  importantly,  IAS  estimates  provide  insight  into  the nature  of  the  predictive  mechanisms  that  generate  those  
responses  during language comprehension.  Variability in neural and behavioural responses is well  explained  by  different  combinations  
of  the  representational  and  temporal  resolution  of  prediction.   Processing  difficulty  calculated  at  varying representational 
domains reflects known relations to lexical, constructional, and structural levels of linguistic processing, and forecast horizons are 
determined by a combination of experimental task setup and naturalness of the stimulus.   Beyond  enriching  psycholinguistic  models,  
IAS  can  also  provide insights into the information processing mechanisms of computational language models.  
Our analysis of next-word surprisal under the lenses of IAS reveals that,  despite the metric’s seemingly narrow focus on the upcoming word, 
language model surprisal implicitly captures anticipatory processing of multiple future lexical items.}
}

@article{ghaleb-etal-2024-arxiv,
title={Leveraging Speech for Gesture Detection in Multimodal Communication}, 
author={Esam Ghaleb and Ilya Burenko and Marlou Rasenberg and Wim Pouw and Ivan Toni and Peter Uhrig and Anna Wilson and Judith Holler and Asl\i \"Ozy\"urek and Raquel Fern\'andez},
journal = {ArXiv},
year={Preprints},
url={https://arxiv.org/pdf/2404.14952},
note={2024.},
abstract={Gestures are inherent to human interaction and often complement speech in face-to-face communication, forming a multimodal 
communication system. An important task in gesture analysis is detecting a gesture's beginning and end. Research on automatic gesture 
detection has primarily focused on visual and kinematic information to detect a limited set of isolated or silent gestures with low 
variability, neglecting the integration of speech and vision signals to detect gestures that co-occur with speech. 
This work addresses this gap by focusing on co-speech gesture detection, emphasising the synchrony between speech and co-speech hand gestures. 
We address three main challenges: the variability of gesture forms, the temporal misalignment between gesture and speech onsets, 
and differences in sampling rate between modalities. We investigate extended speech time windows and employ separate backbone models 
for each modality to address the temporal misalignment and sampling rate differences. We utilize Transformer encoders in cross-modal 
and early fusion techniques to effectively align and integrate speech and skeletal sequences. The study results show that combining 
visual and speech information significantly enhances gesture detection performance. Our findings indicate that expanding the speech 
buffer beyond visual time segments improves performance and that multimodal integration using cross-modal and early fusion techniques 
outperforms baseline methods using unimodal and late fusion methods. Additionally, we find a correlation between the models' gesture 
prediction confidence and low-level speech frequency features potentially associated with gestures. Overall, the study provides a better 
understanding and detection methods for co-speech gestures, facilitating the analysis of multimodal communication.}
}
