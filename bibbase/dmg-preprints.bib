@article{Bavaresco-deHeerKloots-etal-arxiv-2024,
title={Modelling Multimodal Integration in Human Concept Processing with Vision-and-Language Models},
author={Anna Bavaresco and Marianne de Heer Kloots and Sandro Pezzelle and Raquel Fern\'{a}ndez},
year={Preprints},
url={https://arxiv.org/abs/2407.17914},
note={2024.},
abstract={Representations from deep neural networks (DNNs) have proven remarkably predictive of neural activity involved in both visual and linguistic processing. Despite these successes, most studies to date concern unimodal DNNs, encoding either visual or textual input but not both. Yet, there is growing evidence that human meaning representations integrate linguistic and sensory-motor information. Here we investigate whether the integration of multimodal information operated by current vision-and-language DNN models (VLMs) leads to representations that are more aligned with human brain activity than those obtained by language-only and vision-only DNNs. We focus on fMRI responses recorded while participants read concept words in the context of either a full sentence or an accompanying picture. Our results reveal that VLM representations correlate more strongly than language- and vision-only DNNs with activations in brain areas functionally related to language processing. A comparison between different types of visuo-linguistic architectures shows that recent generative VLMs tend to be less brain-aligned than previous architectures with lower performance on downstream applications. Moreover, through an additional analysis comparing brain vs. behavioural alignment across multiple VLMs, we show that -- with one remarkable exception -- representations that strongly align with behavioural judgments do not correlate highly with brain responses. This indicates that brain similarity does not go hand in hand with behavioural similarity, and vice versa.
}

@article{IAS-psyarxiv-2024,
title={Incremental Alternative Sampling as a Lens into the Temporal and Representational Resolution of Linguistic Prediction},
author={Mario Giulianelli and Sarenne Wallbridge and Ryan Cotterell and Raquel Fern\'{a}ndez},
year={Preprints},
url={https://osf.io/preprints/psyarxiv/fhp84},
note={2024.},
abstract={This study presents a new model of processing difficulty rooted in resource allocation theory, Incremental Alternative Sampling (IAS). Differential difficulty for a linguistic unit is estimated with respect to a set of plausible alter-natives.  Compared to a surprisal-based model, it prescribes a more efficient use of a comprehender’s predicted continuations of partial linguistic stimuli thanks to (i) an expressive representation function that captures different lev-els of linguistic processing and (ii) the bootstrapping of long-horizon prediction error.  Our results show that IAS estimates of processing difficulty, computed with autoregressive language models via Monte Carlo estimation, have greater  predictive  power  than  surprisal  extracted  from  the  same  language models for most neural and behavioural responses under analysis---including reading times, event-related brain potentials, cloze and predictability judgements.   Perhaps  more  importantly,  IAS  estimates  provide  insight  into  the nature  of  the  predictive  mechanisms  that  generate  those  responses  during language comprehension.  Variability in neural and behavioural responses is well  explained  by  different  combinations  of  the  representational  and  temporal  resolution  of  prediction.   Processing  difficulty  calculated  at  varying representational domains reflects known relations to lexical, constructional, and structural levels of linguistic processing, and forecast horizons are deter-mined by a combination of experimental task setup and naturalness of the stimulus.   Beyond  enriching  psycholinguistic  models,  IAS  can  also  provide insights into the information processing mechanisms of computational language models.  Our analysis of next-word surprisal under the lenses of IAS reveals that,  despite the metric’s seemingly narrow focus on the upcoming word, language model surprisal implicitly captures anticipatory processing of multiple future lexical items.
}

@article{judgebench-arxiv-2024,
title={LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks},
author={Anna Bavaresco and Raffaella Bernardi and Leonardo Bertolazzi and Desmond Elliott and Raquel Fern\'{a}ndez and Albert Gatt and Esam Ghaleb and Mario Giulianelli and Michael Hanna and Alexander Koller and Andr\'{e} F. T. Martins and Philipp Mondorf and Vera Neplenbroek and Sandro Pezzelle and Barbara Plank and David Schlangen and Alessandro Suglia and Aditya K. Surikuchi and Ece Takmaz and Alberto Testoni},
year={Preprints},
url={https://arxiv.org/abs/2406.18403},
url_github = {https://github.com/dmg-illc/JUDGE-BENCH},
note={2024.},
abstract={There is an increasing trend towards evaluating NLP models with LLM-generated judgments instead of human judgments. In the absence of a comparison against human data, this raises concerns about the validity of these evaluations; in case they are conducted with proprietary models, this also raises concerns over reproducibility. We provide JUDGE-BENCH, a collection of 20 NLP datasets with human annotations, and comprehensively evaluate 11 current LLMs, covering both open-weight and proprietary models, for their ability to replicate the annotations. Our evaluations show that each LLM exhibits a large variance across datasets in its correlation to human judgments. We conclude that LLMs are not yet ready to systematically replace human judges in NLP.
}

@article{ghaleb-etal-2024-arxiv,
      title={Leveraging Speech for Gesture Detection in Multimodal Communication}, 
      author={Esam Ghaleb and Ilya Burenko and Marlou Rasenberg and Wim Pouw and Ivan Toni and Peter Uhrig and Anna Wilson and Judith Holler and Asl\i \"Ozy\"urek and Raquel Fern\'andez},
      year={Preprints},
      url={https://arxiv.org/pdf/2404.14952},
      note={2024.},
      abstract={Gestures are inherent to human interaction and often complement speech in face-to-face communication, forming a multimodal communication system. An important task in gesture analysis is detecting a gesture's beginning and end. Research on automatic gesture detection has primarily focused on visual and kinematic information to detect a limited set of isolated or silent gestures with low variability, neglecting the integration of speech and vision signals to detect gestures that co-occur with speech. This work addresses this gap by focusing on co-speech gesture detection, emphasising the synchrony between speech and co-speech hand gestures. We address three main challenges: the variability of gesture forms, the temporal misalignment between gesture and speech onsets, and differences in sampling rate between modalities. We investigate extended speech time windows and employ separate backbone models for each modality to address the temporal misalignment and sampling rate differences. We utilize Transformer encoders in cross-modal and early fusion techniques to effectively align and integrate speech and skeletal sequences. The study results show that combining visual and speech information significantly enhances gesture detection performance. Our findings indicate that expanding the speech buffer beyond visual time segments improves performance and that multimodal integration using cross-modal and early fusion techniques outperforms baseline methods using unimodal and late fusion methods. Additionally, we find a correlation between the models' gesture prediction confidence and low-level speech frequency features potentially associated with gestures. Overall, the study provides a better understanding and detection methods for co-speech gestures, facilitating the analysis of multimodal communication.}
}
